{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION: libraries, parameters, network...\n",
    "from keras.models import Sequential      # One layer after the other\n",
    "from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs\n",
    "from collections import deque            # For storing moves \n",
    "import numpy as np\n",
    "from keras.layers import Dense,Dropout\n",
    "from tqdm import tqdm\n",
    "from keras import optimizers\n",
    "np.random.seed(7)\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=4,activation='relu'))\n",
    "model.add(Dense(256,kernel_initializer='normal', activation='relu'))\n",
    "Dropout(0.8)\n",
    "model.add(Dense(128,kernel_initializer='normal', activation='relu'))\n",
    "Dropout(0.8)\n",
    "model.add(Dense(512,kernel_initializer='normal', activation='relu'))\n",
    "Dropout(0.8)\n",
    "model.add(Dense(128,kernel_initializer='normal', activation='relu'))\n",
    "Dropout(0.8)\n",
    "model.add(Dense(1,kernel_initializer='normal',activation='sigmoid'))    # Same number of outputs as possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observetime = 500                        # Number of timesteps we will be acting on the game and observing results\n",
    "epsilon = 0.7                            # Probability of doing a random move\n",
    "score_requirement = 50                   # score requirement  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training datasets correspoding to states and actions\n",
    "\n",
    "def population(initial_games): \n",
    "    accepted_scores=[]\n",
    "    training_data=[]\n",
    "    scores=[]\n",
    "    for _ in tqdm(range(initial_games)):\n",
    "        observation = env.reset()\n",
    "        obs = observation\n",
    "        state = obs\n",
    "        score=0\n",
    "        game_memory=[]\n",
    "        prev_observation=[]\n",
    "        for _ in range(observetime):           \n",
    "            env.render()\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = random.randint(0,1)\n",
    "            else:\n",
    "                state = np.squeeze(state).reshape(1,4)\n",
    "                Q = model.predict(state)        \n",
    "                action = np.int(np.round(Q))\n",
    "            observation_new, reward, done, info = env.step(action)     # See state of the game, reward... after performing the action\n",
    "            obs_new = observation_new          # (Formatting issues)\n",
    "            state_new = obs_new     # Update the input with the new state of the game\n",
    "            if len(prev_observation)>0:\n",
    "                game_memory.append([prev_observation,action])\n",
    "            state = state_new         # Update state\n",
    "            prev_observation = state_new\n",
    "            score+=1\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "        if score>=score_requirement:\n",
    "            accepted_scores.append(scores)\n",
    "            for data in game_memory:\n",
    "                if data[1]==1:\n",
    "                    output=[1]\n",
    "                else: \n",
    "                    output=[0]\n",
    "                \n",
    "                training_data.append([data[0],output])\n",
    "                                \n",
    "                \n",
    "        env.reset()\n",
    "        scores.append(score)\n",
    "    \n",
    "    np.save('mem.npz',training_data)\n",
    "    print(\"Avg accepted score: \",np.mean(accepted_scores))\n",
    "    print(\"Median accepted score: \",np.median(accepted_scores))\n",
    "    env.close()     \n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = population(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in training_data])\n",
    "y = [i[1] for i in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X,y,batch_size=100,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play():#import gym                                # To train our network\n",
    "    env = gym.make('CartPole-v0') \n",
    "    observation = env.reset()\n",
    "    obs = observation\n",
    "    state = obs\n",
    "    done = False\n",
    "    tot_reward = 0.0\n",
    "    while not done:\n",
    "        env.render()                    # Uncomment to see game running\n",
    "        state = np.squeeze(state).reshape(1,4)\n",
    "        Q = model.predict(state)        \n",
    "        action = np.int(np.round(Q))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        obs = observation\n",
    "        state = obs    \n",
    "        tot_reward += reward\n",
    "    env.close()\n",
    "    print('Game ended! Total reward: {}'.format(tot_reward))\n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
